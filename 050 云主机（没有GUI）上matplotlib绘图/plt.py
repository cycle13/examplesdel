'''Trains a simple convnet on the MNIST dataset.

Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''
from __future__ import print_function
import keras
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

history1 =  {'acc': [0.9424166666666667, 0.9772833333333333, 0.98205, 0.9845666666666667, 0.9856166666666667, 0.9858, 0.98645, 0.98705, 0.9876, 0.9877166666666667, 0.98865, 0.98895, 0.9893333333333333, 0.9898166666666667, 0.9899166666666667, 0.9900666666666667, 0.9908166666666667, 0.9906166666666667, 0.9906666666666667, 0.9907833333333333, 0.9912833333333333, 0.9910666666666667, 0.9913833333333333, 0.9916833333333334, 0.99205, 0.99185, 0.99235, 0.99195, 0.99195, 0.9923, 0.9923, 0.9926666666666667, 0.9932, 0.9927166666666667, 0.9931166666666666, 0.99335, 0.9931, 0.9931666666666666, 0.9925666666666667, 0.9937166666666667, 0.9929666666666667, 0.9933666666666666, 0.9936, 0.9934166666666666, 0.9932333333333333, 0.9937333333333334, 0.9935333333333334, 0.9941666666666666, 0.9943666666666666, 0.9941166666666666, 0.9938, 0.9939166666666667, 0.9939166666666667, 0.9942, 0.9940666666666667, 0.994, 0.9946, 0.9947166666666667, 0.9942333333333333, 0.9945666666666667, 0.9948, 0.9952833333333333, 0.9951333333333333, 0.9954666666666667, 0.99495, 0.99535, 0.9952666666666666, 0.9951333333333333, 0.9949833333333333, 0.9952666666666666, 0.99525, 0.9953333333333333, 0.9954166666666666, 0.9955333333333334, 0.9948666666666667, 0.9953833333333333, 0.99555, 0.9956666666666667, 0.9958, 0.996, 0.9959833333333333, 0.99605, 0.9961, 0.9962666666666666, 0.9962833333333333, 0.9958833333333333, 0.9962333333333333, 0.9958333333333333, 0.9964333333333333, 0.99635, 0.99655, 0.9966666666666667, 0.9965, 0.9964166666666666, 0.9962333333333333, 0.9961333333333333, 0.9961666666666666, 0.9965333333333334, 0.9958333333333333, 0.9965666666666667], 'loss': [0.18769653058089317, 0.07800343484431506, 0.06115997842356252, 0.05275500664145996, 0.050185016182682014, 0.04697176758535012, 0.045945558000390886, 0.04501083613615289, 0.04560745566095575, 0.04332250884943642, 0.040011471168270994, 0.0374909460851554, 0.036809819434317374, 0.03575493174208944, 0.03660612183416573, 0.03462341962011318, 0.034325319030940105, 0.033158512386819106, 0.03143419492405131, 0.03326221046819604, 0.03078490316879482, 0.031200312172323174, 0.031213362707786775, 0.029689202077365553, 0.028700835691296563, 0.029838571328080193, 0.027613854080032373, 0.028225119234151254, 0.027608893293076683, 0.026855285601381486, 0.02651731224565789, 0.025974365828169175, 0.026557757494911856, 0.026273735772527743, 0.02450024239804067, 0.02531997420966433, 0.025940574101875974, 0.024607626242382247, 0.025334246335124346, 0.02233130974388909, 0.026519736287537093, 0.024499717008147733, 0.024247740847705396, 0.024086956775717757, 0.024769188386757127, 0.022734281843158774, 0.024381759109900532, 0.021079132005534545, 0.01950708677220125, 0.022820018706627754, 0.0230111795691016, 0.021589200840195676, 0.022161466305971392, 0.021481323976754334, 0.02009874985375947, 0.02215188479221336, 0.020666998243486546, 0.019971152210153498, 0.02087515887182472, 0.020381485286694017, 0.01930399094593672, 0.018511130393327965, 0.01770068600321593, 0.017288610382862998, 0.018216053290131807, 0.016970928326559276, 0.017170248836996166, 0.017484947482889333, 0.017233981787766802, 0.01703479163617031, 0.015858000967413893, 0.0161436304206538, 0.016188664999755777, 0.01638144086751012, 0.01765281781879816, 0.016307891428631743, 0.01601778759252853, 0.015936003811925926, 0.014563137230554852, 0.015215767268536358, 0.015211293017602255, 0.014873772994049015, 0.01404845383597176, 0.013643547235664497, 0.014101293444324392, 0.014989869088569398, 0.013857718653178806, 0.014077008010204593, 0.01271580589373926, 0.01331848718849595, 0.012341024620883248, 0.012524845623222435, 0.012462614175099558, 0.013133331946942871, 0.012466247032609514, 0.013815488434020721, 0.013018262989269223, 0.012812782662101101, 0.01466261511096155, 0.012361271806908401], 'val_acc': [0.9825, 0.9872, 0.9868, 0.9898, 0.989, 0.9894, 0.9914, 0.9906, 0.9903, 0.9916, 0.9893, 0.9913, 0.9909, 0.9913, 0.9904, 0.9906, 0.9911, 0.9907, 0.9893, 0.9922, 0.9912, 0.9925, 0.9909, 0.9922, 0.9923, 0.9901, 0.9926, 0.9915, 0.9919, 0.9921, 0.9922, 0.9917, 0.9908, 0.9917, 0.9908, 0.9908, 0.9893, 0.9921, 0.9911, 0.9905, 0.9916, 0.9909, 0.9911, 0.9912, 0.991, 0.992, 0.9921, 0.9918, 0.9915, 0.991, 0.9901, 0.9919, 0.9909, 0.9927, 0.9908, 0.9902, 0.9906, 0.9912, 0.9915, 0.9911, 0.9901, 0.9906, 0.9921, 0.9905, 0.9918, 0.9903, 0.9908, 0.9889, 0.992, 0.9915, 0.9928, 0.9908, 0.9903, 0.9912, 0.9914, 0.9922, 0.9921, 0.9914, 0.9919, 0.9914, 0.9911, 0.9909, 0.9908, 0.9912, 0.9919, 0.9922, 0.9918, 0.9907, 0.992, 0.9919, 0.9909, 0.9909, 0.9912, 0.9917, 0.9903, 0.9915, 0.9913, 0.9912, 0.9894, 0.9911], 'val_loss': [0.05063411471816944, 0.0382719242320105, 0.04110821118286694, 0.030199999534005473, 0.03263519335850524, 0.03314385887843091, 0.02864859925844794, 0.030193166385164658, 0.030510199550331164, 0.028807554323667318, 0.03604353337612792, 0.0307114800641888, 0.029948682288473857, 0.03229304125790477, 0.02784880581143807, 0.03102840149718959, 0.030224836466998476, 0.033030934309888425, 0.037929614781646526, 0.028917739075708233, 0.03222517820548564, 0.028908987757660407, 0.03453149877950627, 0.03268179524659954, 0.03166754693030743, 0.033305814327437476, 0.030695482261518373, 0.034232285152726034, 0.032336599292476606, 0.03231190175681218, 0.03222258142285068, 0.034309933663438597, 0.03691000014852921, 0.032990774927318764, 0.039498896857566114, 0.03859175924233532, 0.03539846663720673, 0.0354042073912497, 0.0363365451222051, 0.04085749754177448, 0.03572675031619633, 0.033250018035922764, 0.040561227727922415, 0.036776744155929325, 0.03710678677897526, 0.030737545080703057, 0.03172014537384698, 0.032596176172327206, 0.040060827758547496, 0.04072074413925548, 0.03623220371187781, 0.03512677412254716, 0.03364389111817127, 0.03434784067246278, 0.04190953772656003, 0.04297898575029722, 0.05011029847276473, 0.04270473987771806, 0.037811606789863936, 0.03827195561302224, 0.04474652452649752, 0.03939145543552536, 0.0346365473926488, 0.038003814596360964, 0.03714025175043278, 0.03951436703844647, 0.03934369169284869, 0.048993829815991964, 0.032090623674464226, 0.035743313828706456, 0.03038362368859698, 0.043381799199344866, 0.0416919513721768, 0.0426769172279106, 0.03805377204851743, 0.03463103457620127, 0.03271338744961722, 0.046082506407134134, 0.03648052556391453, 0.04312257318869979, 0.0418313067462042, 0.040326199490817, 0.0433388194496812, 0.03902695872971699, 0.03757654049049402, 0.03966071478243261, 0.03635925747007309, 0.04315121203485496, 0.0352652264791323, 0.03688084729961215, 0.036864244983735035, 0.04321351153759847, 0.03892892799834194, 0.04183796714906957, 0.048218286106662074, 0.05219560420892212, 0.035711281396093544, 0.04960273190421858, 0.04568789589404769, 0.040051573394744355]}
epoch1 =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
print('history1 = ',history1)
print('epoch1 = ',epoch1)			  

history2 =  {'acc': [0.9955166666666667, 0.9963666666666666, 0.9969, 0.9969, 0.9966166666666667, 0.9972166666666666, 0.9975, 0.99735, 0.9970666666666667, 0.9976333333333334, 0.9974833333333334, 0.99725, 0.9972, 0.9973333333333333, 0.99735, 0.9977833333333334, 0.9975166666666667, 0.99735, 0.9974833333333334, 0.9975, 0.99795, 0.9978166666666667, 0.9973166666666666, 0.9976, 0.99805, 0.99785, 0.99785, 0.9977833333333334, 0.9978333333333333, 0.9981833333333333, 0.9978166666666667, 0.9978333333333333, 0.9978, 0.9977333333333334, 0.9977, 0.99765, 0.9977666666666667, 0.99805, 0.9982666666666666, 0.99785, 0.9978833333333333, 0.9983, 0.9981833333333333, 0.9982166666666666, 0.998, 0.9981333333333333, 0.9983166666666666, 0.9980666666666667, 0.9980833333333333, 0.9980333333333333, 0.99815, 0.9984166666666666, 0.9983666666666666, 0.9984, 0.9983, 0.9983666666666666, 0.99835, 0.9984166666666666, 0.9980666666666667, 0.998, 0.9982333333333333, 0.9984166666666666, 0.99835, 0.99865, 0.9982833333333333, 0.9983333333333333, 0.9986166666666667, 0.9985833333333334, 0.9986166666666667, 0.99825, 0.9983, 0.9984333333333333, 0.9986833333333334, 0.9982833333333333, 0.9983333333333333, 0.99855, 0.9985333333333334, 0.99845, 0.9984833333333333, 0.9981666666666666, 0.9985166666666667, 0.9983333333333333, 0.9987166666666667, 0.9985666666666667, 0.99855, 0.9985333333333334, 0.9986166666666667, 0.9985, 0.99855, 0.9986833333333334, 0.9982333333333333, 0.99865, 0.9986166666666667, 0.9984166666666666, 0.99875, 0.9987, 0.99835, 0.9983833333333333, 0.9984166666666666, 0.9985666666666667], 'loss': [0.3038675465583801, 0.2894287046511968, 0.2844860405842463, 0.2796550591548284, 0.28262437153657277, 0.2758182296911875, 0.2707348935921987, 0.27514393433729806, 0.2760874286651611, 0.2676667750438054, 0.2724343189160029, 0.2722002764383952, 0.2713695697466532, 0.2717582205851873, 0.27236011906464896, 0.2647692502419154, 0.2681902792533239, 0.27052533059914907, 0.26936129400730136, 0.26948596681753795, 0.2618932506720225, 0.26451853992144264, 0.27267961943149566, 0.2676853730281194, 0.26090871827602385, 0.26374680454730987, 0.2648761959791183, 0.2644145738363266, 0.2636583781162898, 0.2581843469142914, 0.26244072533448537, 0.2637757329146067, 0.2637149457534154, 0.26444051659901935, 0.2641139776388804, 0.2661576905647914, 0.2643339457035065, 0.2600930026213328, 0.256716130065918, 0.2635205726067225, 0.26151334354082745, 0.2572421467542648, 0.25782636365095774, 0.25746309512456256, 0.2597876801331838, 0.2588365065495173, 0.25649501438935596, 0.2599703318039576, 0.25822175441582995, 0.2593438729763031, 0.25858189249833424, 0.25470755393505096, 0.2547394645690918, 0.2548217393875122, 0.25738487763404844, 0.25594282806714375, 0.2556231837352117, 0.2551388910452525, 0.2585396142085393, 0.259387627585729, 0.25705053910414377, 0.25406868409315747, 0.256028796450297, 0.25128802018960317, 0.25648720347881315, 0.2559603317101796, 0.2519159399112066, 0.25191619478861493, 0.2504983579158783, 0.2571686144669851, 0.25533418520291645, 0.25454792540073395, 0.25022978808879853, 0.2561418264865875, 0.25506267053286236, 0.2516080105702082, 0.25282643837134045, 0.252530880300204, 0.25267402987480164, 0.2570744471391042, 0.2522716714779536, 0.2550819706519445, 0.24946491953531902, 0.25150839853286744, 0.25226876371701557, 0.25161847115357716, 0.2512176449139913, 0.2523434052069982, 0.25216613563696544, 0.24960242014726003, 0.25596573173205056, 0.2510485898971558, 0.25147339934508006, 0.25355000872612, 0.24985361746152243, 0.2503647914250692, 0.2541953933318456, 0.2538106077750524, 0.2545000426530838, 0.2519575326919556], 'val_acc': [0.9906, 0.9924, 0.9915, 0.9914, 0.992, 0.9913, 0.9925, 0.9932, 0.992, 0.9922, 0.9916, 0.992, 0.9916, 0.9925, 0.9927, 0.9921, 0.9919, 0.9915, 0.9909, 0.9915, 0.9916, 0.9925, 0.9915, 0.9923, 0.9922, 0.9915, 0.9923, 0.9917, 0.9925, 0.9915, 0.9928, 0.9914, 0.9922, 0.9914, 0.9916, 0.9916, 0.9913, 0.9927, 0.9919, 0.9915, 0.9915, 0.9916, 0.9922, 0.9923, 0.9907, 0.9923, 0.9919, 0.9919, 0.9927, 0.9903, 0.9925, 0.992, 0.9919, 0.9926, 0.9916, 0.992, 0.9917, 0.9927, 0.9917, 0.9926, 0.9914, 0.9916, 0.9921, 0.9913, 0.9925, 0.992, 0.9919, 0.9919, 0.9926, 0.9924, 0.9927, 0.9926, 0.9931, 0.9932, 0.9928, 0.9912, 0.9915, 0.9918, 0.9914, 0.9923, 0.992, 0.9925, 0.9925, 0.9913, 0.9926, 0.9919, 0.9924, 0.9922, 0.9924, 0.9929, 0.993, 0.9927, 0.9929, 0.9918, 0.9927, 0.9928, 0.9925, 0.9925, 0.9931, 0.9926], 'val_loss': [0.372787429022789, 0.3465993968486786, 0.3559832444190979, 0.36106483597755434, 0.34522133030891416, 0.3587197118282318, 0.34387199563980103, 0.3338906853199005, 0.34597729206085204, 0.34541769309043885, 0.35544726920127867, 0.3440274303913117, 0.35078165130615235, 0.3451554035663605, 0.3402187351703644, 0.34458481674194336, 0.35120919489860536, 0.35249656867980955, 0.36236519708633425, 0.3525913935661316, 0.3510757940769196, 0.34030945529937745, 0.3530057960033417, 0.3392881209373474, 0.3435988008022308, 0.3542726428985596, 0.3457647687911987, 0.35357318091392514, 0.3422030526161194, 0.3529231000423431, 0.33753214926719666, 0.355350666809082, 0.34622107219696047, 0.3535248135089874, 0.3511140303134918, 0.35324653787612914, 0.35768128843307495, 0.33602724919319155, 0.3534246561050415, 0.35383299379348754, 0.3521455334186554, 0.3505999120235443, 0.34497730054855347, 0.34377267622947694, 0.3631690599918366, 0.3421375227451324, 0.34846349749565125, 0.34695397868156436, 0.3359388494491577, 0.3714632269382477, 0.3369950187206268, 0.34547010474205014, 0.34750959153175354, 0.34111285252571105, 0.35373965878486635, 0.3482714569091797, 0.35177599005699156, 0.33671383323669435, 0.3522419891357422, 0.3409789737701416, 0.35780737590789796, 0.35231388854980467, 0.3465603624343872, 0.3562118108272552, 0.34214628133773806, 0.34323515973091123, 0.3509936061859131, 0.34743922004699707, 0.33775915904045106, 0.3412524167060852, 0.338478587436676, 0.3391042097091675, 0.3339868336677551, 0.330578795003891, 0.33551060090065005, 0.3589534411907196, 0.35232890620231627, 0.34666263389587404, 0.3537547061443329, 0.34047220220565794, 0.3459710605621338, 0.33779191040992734, 0.3390142132759094, 0.35604982719421385, 0.3399991213798523, 0.3480793526649475, 0.3420542693138123, 0.342559233045578, 0.34042377071380614, 0.3363857092380524, 0.3321580376625061, 0.3369203851699829, 0.3342936293125153, 0.35024715065956114, 0.33459020199775696, 0.3341118111133575, 0.3417974027633667, 0.3395355637550354, 0.3320516231060028, 0.3376035189628601]}
epoch2 =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
print('history2 = ',history2)
print('epoch2 = ',epoch2)


# plot history
plt.title("model performace")
plt.plot(epoch1,history1['loss'], label='train_loss')
plt.plot(epoch1,history1['val_loss'], label='test_loss')
plt.plot(epoch1,history1['acc'], label='train_acc')
plt.plot(epoch1,history1['val_acc'], label='test_acc')

plt.plot(epoch2,history2['loss'], label='my_train_loss')
plt.plot(epoch2,history2['val_loss'], label='my_test_loss')
plt.plot(epoch2,history2['acc'], label='my_train_acc')
plt.plot(epoch2,history2['val_acc'], label='my_test_acc')

plt.ylabel("loss or acc")
plt.xlabel("epochs")
plt.legend()
plt.savefig('history.png')
#plt.show()

